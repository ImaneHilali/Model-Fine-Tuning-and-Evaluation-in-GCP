{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966ec020-9959-44a3-8a93-d9f7c1213d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.9.11)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.25.2)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (2.1.0)\n",
      "Collecting nltk (from rouge-score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.4)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f178902d6a5bfbea5b36cb332b483c5982a11ca9d57b6b99fb5f6c1b7747553c\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: portalocker, nltk, lxml, sacrebleu, rouge-score\n",
      "Successfully installed lxml-5.3.0 nltk-3.9.1 portalocker-2.10.1 rouge-score-0.1.2 sacrebleu-2.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96046a4a-877f-4d8e-b6b4-305147ae44d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "Successfully installed tokenizers-0.20.1 transformers-4.45.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb7c1d-733f-4013-a1dc-2c02a3e3b2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from google.cloud import storage\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Path to the checkpoint folder in GCS\n",
    "checkpoint_path = 'gs://translation-datasets/nllb-finetuned/nllb-finetuned/checkpoint-72000'\n",
    "local_checkpoint_path = '/tmp/checkpoint-72000'\n",
    "\n",
    "# Function to download model files from GCS to local storage\n",
    "def download_model_from_gcs(gcs_path, local_path):\n",
    "    client = storage.Client()\n",
    "    bucket_name, folder_path = gcs_path.replace('gs://', '').split('/', 1)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=folder_path)\n",
    "\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    for blob in blobs:\n",
    "        local_file_path = os.path.join(local_path, blob.name.split('/')[-1])\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print(f\"Downloaded {blob.name} to {local_file_path}\")\n",
    "\n",
    "# Download the model from GCS\n",
    "download_model_from_gcs(checkpoint_path, local_checkpoint_path)\n",
    "\n",
    "# Load the fine-tuned model and tokenizer from the local path\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_checkpoint_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_checkpoint_path)\n",
    "\n",
    "# Move the model to the appropriate device, The device is determined based on whether a GPU is available 'cuda' or not 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to load the JSONL test data from GCS and limit to 10 examples\n",
    "def load_jsonl(file_path, max_examples=30):\n",
    "    test_data = []\n",
    "    client = storage.Client()\n",
    "    bucket_name, blob_name = file_path.replace('gs://', '').split('/', 1)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    content = blob.download_as_text(encoding='utf-8')\n",
    "\n",
    "    # Process the content and limit to max_examples\n",
    "    for i, line in enumerate(content.splitlines()):\n",
    "        if i >= max_examples:\n",
    "            break\n",
    "        test_data.append(json.loads(line))\n",
    "    return test_data\n",
    "\n",
    "#def load_jsonl(file_path):\n",
    "#    test_data = []\n",
    "#    client = storage.Client()\n",
    "#    bucket_name, blob_name = file_path.replace('gs://', '').split('/', 1)\n",
    "#    bucket = client.bucket(bucket_name)\n",
    "#    blob = bucket.blob(blob_name)\n",
    "#    content = blob.download_as_text(encoding='utf-8')\n",
    "\n",
    "    # Process the content and load all examples\n",
    "#    for line in content.splitlines():\n",
    "#        test_data.append(json.loads(line))\n",
    "    \n",
    "#    return test_data\n",
    "\n",
    "# Function to perform translation\n",
    "def translate_text(model, tokenizer, text, source_lang_code, target_lang_code):\n",
    "    tokenizer.src_lang = source_lang_code\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate translation\n",
    "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(target_lang_code)\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Load the test dataset\n",
    "test_data_path = 'gs://translation-datasets/test.jsonl'\n",
    "test_data = load_jsonl(test_data_path,max_examples=30)\n",
    "\n",
    "# Perform translation and store results\n",
    "reference_texts = []\n",
    "generated_texts = []\n",
    "\n",
    "for sample in test_data:\n",
    "    source_text = sample['source_text']\n",
    "    target_text = sample['target_text']\n",
    "    source_lang_code = sample['source_lang']\n",
    "    target_lang_code = sample['target_lang']\n",
    "\n",
    "    # Perform translation\n",
    "    translated_text = translate_text(model, tokenizer, source_text, source_lang_code, target_lang_code)\n",
    "\n",
    "    # Store the reference and generated texts for evaluation\n",
    "    reference_texts.append(target_text)\n",
    "    generated_texts.append(translated_text)\n",
    "\n",
    "# Function to evaluate BLEU score\n",
    "def evaluate_bleu(reference_texts, generated_texts):\n",
    "    bleu = corpus_bleu(generated_texts, [reference_texts])\n",
    "    return bleu.score\n",
    "\n",
    "# Function to evaluate ROUGE scores\n",
    "def evaluate_rouge(reference_texts, generated_texts):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "\n",
    "    for ref, gen in zip(reference_texts, generated_texts):\n",
    "        score = scorer.score(ref, gen)\n",
    "        rouge_scores.append(score)\n",
    "\n",
    "    # Return the average ROUGE scores\n",
    "    avg_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    avg_rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    avg_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "\n",
    "    return {\n",
    "        \"ROUGE-1\": avg_rouge1,\n",
    "        \"ROUGE-2\": avg_rouge2,\n",
    "        \"ROUGE-L\": avg_rougeL\n",
    "    }\n",
    "\n",
    "# Evaluate BLEU score\n",
    "bleu_score = evaluate_bleu(reference_texts, generated_texts)\n",
    "print(f\"BLEU score: {bleu_score}\")\n",
    "\n",
    "# Evaluate ROUGE scores\n",
    "rouge_scores = evaluate_rouge(reference_texts, generated_texts)\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead786b2-71d3-450d-95dd-e0b852dba653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, how are you?\n",
      "Translated: Bonjour, comment allez-vous?\n",
      "\n",
      "Original: This is a test of the NLLB model.\n",
      "Translated: Ceci est un test du modèle NLLB.\n",
      "\n",
      "Original: Let's see how well it performs.\n",
      "Translated: Voyons si ça marche.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "local_checkpoint_path = '/tmp/checkpoint-72000'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_checkpoint_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_checkpoint_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "source_lang = \"eng_Latn\"\n",
    "target_lang = \"fra_Latn\"\n",
    "\n",
    "def translate_batch_nllb(texts, source_lang, target_lang):\n",
    "\n",
    "    tokenizer.src_lang = source_lang\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(target_lang)\n",
    "\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "\n",
    "    translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translations\n",
    " \n",
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This is a test of the NLLB model.\",\n",
    "    \"Let's see how well it performs.\"\n",
    "]\n",
    "\n",
    "translated_texts = translate_batch_nllb(texts, source_lang, target_lang)\n",
    "\n",
    "for i, translation in enumerate(translated_texts):\n",
    "    print(f\"Original: {texts[i]}\")\n",
    "    print(f\"Translated: {translation}\\n\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
